# LiteLLM Proxy Configuration for ccproxy - HYBRID ROUTING (FIXED)
# Fixed: Correct max_tokens + health checks fully disabled

general_settings:
  litellm_settings:
    drop_params: true
    set_verbose: false
  # DISABLE health checks completely
  health_check_interval: 0
  health_check_timeout: 0

server:
  host: "127.0.0.1"
  port: 8081

# Hybrid Model Configuration (5 MODELS)
model_list:
  # PHASE 0 ARCHITECT: Chief Architect (Real Claude API)
  - model_name: claude-opus-4
    litellm_params:
      model: anthropic/claude-opus-4-20250514
      api_key: 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
      max_tokens: 8192  # FIXED: Was 200000, claude-opus-4 max is 8192
      stream: true

  # DEFAULT: Normal Claude Code conversations (FREE - Local Ollama)
  - model_name: claude-sonnet-4-5
    litellm_params:
      model: ollama/qwen-quality-128k:latest
      api_base: http://localhost:11434
      max_tokens: 131072
      stream: true

  # PHASE 1 CODING: Agents 1-10
  - model_name: claude-3-5-sonnet
    litellm_params:
      model: ollama/qwen2.5-coder:32b-instruct
      api_base: http://localhost:11434
      max_tokens: 32768
      stream: true

  # PHASE 1 LIGHTWEIGHT: Agent 11
  - model_name: claude-3-haiku
    litellm_params:
      model: ollama/qwen-fast:latest
      api_base: http://localhost:11434
      max_tokens: 32768
      stream: true

  # PHASE 2 REASONING: Agents 13-15
  - model_name: gpt-4
    litellm_params:
      model: ollama/qwen-quality-128k:latest
      api_base: http://localhost:11434
      max_tokens: 131072
      stream: true

logging:
  log_file: /Users/brent/ccproxy/logs/litellm.log
  log_level: INFO
  request_log: true
  response_log: false

router_settings:
  timeout: 300
  num_retries: 0
  routing_strategy: "simple-shuffle"
  # Disable all health check features
  disable_cooldowns: true
  allowed_fails: 999999
  cooldown_time: 0
