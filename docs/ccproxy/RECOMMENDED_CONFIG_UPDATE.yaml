# RECOMMENDED CONFIG UPDATE FOR config.yaml
# Based on available Qwen Coder models analysis
# Date: 2025-11-04

# REPLACE the current model_list section with this:

model_list:
  # ============================================================================
  # QWEN CODER MODELS - Three tiers for different use cases
  # ============================================================================

  # Fast 7B model - Speed optimized
  - model_name: qwen-fast
    litellm_params:
      model: ollama/qwen2.5-coder:7b-instruct
      api_base: http://localhost:11434
      max_tokens: 32768
      temperature: 0.7
      stream: true

  # Quality 32B model - Standard context
  - model_name: qwen-quality
    litellm_params:
      model: ollama/qwen2.5-coder:32b-instruct
      api_base: http://localhost:11434
      max_tokens: 32768
      temperature: 0.7
      stream: true

  # NEWEST: 32B model with 128k context - For large codebases
  - model_name: qwen-latest
    litellm_params:
      model: ollama/qwen2.5-coder:32b-instruct-128k
      api_base: http://localhost:11434
      max_tokens: 131072  # 128k context window
      temperature: 0.7
      stream: true

  # ============================================================================
  # OpenAI-COMPATIBLE ALIASES - Map to newest model for best quality
  # ============================================================================

  - model_name: claude-3-5-sonnet
    litellm_params:
      model: ollama/qwen2.5-coder:32b-instruct-128k  # Changed from llama2
      api_base: http://localhost:11434
      max_tokens: 131072  # Full 128k context
      temperature: 0.7
      stream: true

  - model_name: gpt-4
    litellm_params:
      model: ollama/qwen2.5-coder:32b-instruct-128k  # Changed from llama2
      api_base: http://localhost:11434
      max_tokens: 131072  # Full 128k context
      temperature: 0.7
      stream: true

  # ============================================================================
  # DIRECT OLLAMA ACCESS - For explicit model selection
  # ============================================================================

  - model_name: ollama/qwen-fast
    litellm_params:
      model: ollama/qwen2.5-coder:7b-instruct
      api_base: http://localhost:11434
      max_tokens: 32768
      temperature: 0.7
      stream: true

  - model_name: ollama/qwen-quality
    litellm_params:
      model: ollama/qwen2.5-coder:32b-instruct
      api_base: http://localhost:11434
      max_tokens: 32768
      temperature: 0.7
      stream: true

  - model_name: ollama/qwen-latest
    litellm_params:
      model: ollama/qwen2.5-coder:32b-instruct-128k
      api_base: http://localhost:11434
      max_tokens: 131072
      temperature: 0.7
      stream: true

  # Legacy compatibility (if still needed)
  - model_name: ollama/llama2
    litellm_params:
      model: ollama/llama2
      api_base: http://localhost:11434
      stream: true

# ============================================================================
# KEY CHANGES FROM PREVIOUS CONFIG:
# ============================================================================
#
# 1. ❌ REMOVED: References to ollama/llama2 as primary model
# 2. ✅ ADDED: Three-tier Qwen Coder setup (7B, 32B, 32B-128k)
# 3. ✅ ADDED: qwen-latest (32B-128k) as the newest/best model
# 4. ✅ UPDATED: claude-3-5-sonnet alias now uses qwen-latest (128k context)
# 5. ✅ UPDATED: gpt-4 alias now uses qwen-latest (128k context)
# 6. ✅ ADDED: Full 131072 token limit for 128k models
#
# ============================================================================
# MODEL SELECTION GUIDE:
# ============================================================================
#
# Use qwen-fast (7B):
#   - Quick bug fixes
#   - Simple functions
#   - Documentation
#   - Speed is priority
#
# Use qwen-quality (32B):
#   - Medium complexity
#   - Standard context needs
#   - Balance quality/speed
#
# Use qwen-latest (32B-128k):  ← THE "3RD NEWER MODEL"
#   - Complex multi-file projects
#   - Large context requirements
#   - Production-grade code
#   - API integrations
#   - Full-stack development
#
# ============================================================================
# VERIFICATION COMMANDS (Run when server is online):
# ============================================================================
#
# 1. List all Qwen models:
# curl -s -H "Authorization: Bearer <token>" \
#   https://coder.visiquate.com/api/tags | jq '.models[] | select(.name | contains("qwen"))'
#
# 2. Test the 128k model:
# curl -X POST "https://coder.visiquate.com/api/generate" \
#   -H "Authorization: Bearer <token>" \
#   -d '{"model": "qwen2.5-coder:32b-instruct-128k", "prompt": "test", "stream": false}'
#
# 3. Test via ccproxy with alias:
# curl -X POST "https://coder.visiquate.com/v1/chat/completions" \
#   -H "Authorization: Bearer <token>" \
#   -d '{"model": "claude-3-5-sonnet", "messages": [{"role": "user", "content": "Hello"}]}'
#
# ============================================================================

# NOTE: Before applying this config:
# 1. Verify qwen2.5-coder:32b-instruct-128k is pulled on Mac mini
# 2. Test with sample queries
# 3. Monitor memory usage (32B models need ~32GB RAM)
# 4. Backup existing config.yaml first
