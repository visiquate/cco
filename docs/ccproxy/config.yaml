# LiteLLM Proxy Configuration for ccproxy
# Native macOS Deployment
# Location: /Users/coder/ccproxy/config.yaml

# General Settings
general_settings:
  # Master key for LiteLLM admin access (optional, handled by Traefik)
  # master_key: "your-admin-key-here"

  # Completion fallback models (if primary fails)
  # fallback_models:
  #   - "ollama/mistral"

  # Set to true for detailed debugging
  litellm_settings:
    drop_params: true  # Drop unsupported params instead of failing
    set_verbose: false  # Set to true for debugging

# Server Configuration
server:
  host: "127.0.0.1"  # Localhost only (Traefik handles external access)
  port: 8081         # Internal port (not exposed externally)

  # Health check endpoint (used by Traefik)
  # Available at: http://localhost:8081/health

# Model Configuration
model_list:
  # Fast model for quick tasks (7B, 32k context)
  - model_name: ollama/qwen-fast
    litellm_params:
      model: ollama/qwen2.5-coder:7b-instruct
      api_base: http://localhost:11434
      max_tokens: 32768
      stream: true

  # Quality model for complex features (32B, 32k context)
  - model_name: ollama/qwen-quality
    litellm_params:
      model: ollama/qwen2.5-coder:32b-instruct
      api_base: http://localhost:11434
      max_tokens: 32768
      stream: true

  # Latest model with 128k context - BEST FOR CODING (32B, 128k context)
  - model_name: ollama/qwen-latest
    litellm_params:
      model: ollama/qwen2.5-coder:32b-instruct-128k
      api_base: http://localhost:11434
      max_tokens: 131072  # Full 128k context
      stream: true

  # Model aliases for API compatibility
  - model_name: claude-3-5-sonnet  # For quick tasks and docs
    litellm_params:
      model: ollama/qwen2.5-coder:7b-instruct
      api_base: http://localhost:11434
      max_tokens: 32768
      stream: true

  - model_name: gpt-4  # For complex coding (128k context)
    litellm_params:
      model: ollama/qwen2.5-coder:32b-instruct-128k  # Use LATEST
      api_base: http://localhost:11434
      max_tokens: 131072  # Full 128k context
      stream: true

  - model_name: gpt-4-turbo  # Alias for latest 128k model
    litellm_params:
      model: ollama/qwen2.5-coder:32b-instruct-128k
      api_base: http://localhost:11434
      max_tokens: 131072
      stream: true

# Logging Configuration
logging:
  # Log to file for monitoring and debugging
  log_file: /Users/coder/ccproxy/logs/litellm.log
  log_level: INFO  # Options: DEBUG, INFO, WARNING, ERROR, CRITICAL

  # Request/response logging (use with caution in production)
  request_log: true
  response_log: false  # Set to true for debugging (may log sensitive data)

# Router Configuration
router_settings:
  # Timeout settings
  timeout: 300  # 5 minutes for long-running requests

  # Retry settings
  num_retries: 0  # Don't retry (Ollama is local)

  # Routing strategy
  routing_strategy: "simple-shuffle"  # Random selection if multiple models

# Cost Tracking (Optional)
# litellm_settings:
#   success_callback: ["supabase"]  # Track usage in Supabase
#   failure_callback: ["supabase"]

# Environment Variables (Optional)
# environment_variables:
#   OLLAMA_API_BASE: "http://localhost:11434"

# Rate Limiting (Optional - disabled for local use)
# rate_limit:
#   enabled: false

# Callbacks (Optional)
# litellm_settings:
#   callbacks: ["prometheus"]  # Export metrics to Prometheus

# Notes:
# 1. No authentication configured (Traefik handles bearer token auth)
# 2. Bound to localhost only (no external access without Traefik)
# 3. All Ollama models should be pulled before configuration:
#    ollama pull llama2
#    ollama pull mistral
# 4. Add more model aliases as needed for API compatibility
# 5. Adjust log_level to DEBUG for troubleshooting
