//! Model management for embedded LLM
//!
//! Handles downloading, caching, loading, and lifecycle management
//! of the TinyLLaMA GGML model for CRUD classification.

use crate::daemon::hooks::config::HookLlmConfig;
use crate::daemon::hooks::{HookError, HookResult};
use anyhow::Result;
use futures::StreamExt;
use indicatif::{ProgressBar, ProgressStyle};
use reqwest::Client;
use sha2::{Digest, Sha256};
use std::path::{Path, PathBuf};
use std::sync::Arc;
use tokio::sync::Mutex;
use tracing::{debug, info, warn};

/// Loaded LLM model wrapper
///
/// Note: This is a placeholder structure for Phase 1B implementation.
/// The actual LLM integration will be completed once the llm crate API is properly documented.
pub struct LlmModel {
    /// Placeholder for the loaded model instance
    _model_path: PathBuf,
    /// Model configuration
    _config: HookLlmConfig,
}

/// Extract the actual command from the classification prompt
///
/// The prompt format is:
/// ```text
/// Classify this command as EXACTLY ONE of: READ, CREATE, UPDATE, or DELETE
///
/// Command: <ACTUAL_COMMAND>
///
/// Rules:
/// ...
/// ```
///
/// This function extracts just the `<ACTUAL_COMMAND>` part, avoiding false matches
/// against the examples in the rules section.
fn extract_command_from_prompt(prompt: &str) -> String {
    // Look for "Command: " followed by the actual command
    if let Some(start_idx) = prompt.find("Command: ") {
        let after_label = &prompt[start_idx + 9..]; // Skip "Command: "

        // The command continues until we hit a newline
        if let Some(end_idx) = after_label.find('\n') {
            return after_label[..end_idx].trim().to_string();
        } else {
            // No newline found, take rest of string
            return after_label.trim().to_string();
        }
    }

    // Fallback: couldn't parse the prompt format
    String::new()
}

/// Check if a command is a READ operation
///
/// READ operations retrieve/display data without side effects.
fn is_read_operation(command: &str) -> bool {
    // Check command prefix (start of command)
    let starts_with_read = command.starts_with("ls")
        || command.starts_with("cat ")
        || command.starts_with("grep ")
        || command.starts_with("git status")
        || command.starts_with("git log")
        || command.starts_with("git diff")
        || command.starts_with("ps ")
        || command.starts_with("ps")
        || command.starts_with("find ")
        || command.starts_with("head ")
        || command.starts_with("tail ")
        || command.starts_with("docker ps")
        || command.starts_with("docker logs")
        || command.starts_with("docker inspect")
        || command.starts_with("curl ")
        || command.starts_with("wget ");

    // Check for piped READ commands (cat | grep | sort)
    let is_piped_read = command.contains('|') &&
        !command.contains(" > ") &&
        !command.contains(" >> ") &&
        (command.contains("cat ") || command.contains("grep ") ||
         command.contains("sort") || command.contains("uniq"));

    // Curl without output redirect is READ
    let is_curl_read = command.starts_with("curl ") &&
        !command.contains(" -o ") &&
        !command.contains(" > ");

    starts_with_read || is_piped_read || is_curl_read
}

/// Check if a command is a DELETE operation
///
/// DELETE operations remove resources.
fn is_delete_operation(command: &str) -> bool {
    command.starts_with("rm ")
        || command.starts_with("rm -")
        || command.starts_with("rmdir ")
        || command.starts_with("docker rm")
        || command.starts_with("docker rmi")
        || command.starts_with("git branch -d")
        || command.starts_with("git clean")
        || command.starts_with("npm uninstall")
        || command.starts_with("pip uninstall")
        || command.starts_with("cargo uninstall")
        || command.contains("&& rm ")
        || command.contains("&& rm -")
}

/// Check if a command is a CREATE operation
///
/// CREATE operations make new resources.
fn is_create_operation(command: &str) -> bool {
    command.starts_with("touch ")
        || command.starts_with("mkdir ")
        || command.starts_with("git init")
        || command.starts_with("git branch ")
        || command.starts_with("git checkout -b")
        || command.starts_with("docker run")
        || command.starts_with("docker build")
        || command.starts_with("docker create")
        || command.starts_with("npm init")
        || command.starts_with("npm install")
        || command.starts_with("cargo new")
        || command.starts_with("cargo init")
        || command.starts_with("pip install")
        || command.contains("echo ") && command.contains(" > ")
        || command.contains("cat >")
        || command.contains("curl ") && (command.contains(" -o ") || command.contains(" -O"))
}

/// Check if a command is an UPDATE operation
///
/// UPDATE operations modify existing resources.
fn is_update_operation(command: &str) -> bool {
    command.starts_with("git commit")
        || command.starts_with("git add")
        || command.starts_with("git push")
        || command.starts_with("git pull")
        || command.starts_with("git merge")
        || command.starts_with("git rebase")
        || command.starts_with("chmod ")
        || command.starts_with("chown ")
        || command.starts_with("sed -i")
        || command.starts_with("mv ")
        || command.starts_with("cp ")
        || command.starts_with("npm update")
        || command.starts_with("pip install --upgrade")
        || command.starts_with("cargo update")
        || command.contains(" >> ")
        || command.contains("docker restart")
        || command.contains("docker stop")
        || command.contains("docker start")
}

/// Model manager handles lifecycle of the embedded LLM
///
/// Responsibilities:
/// - Download model from HuggingFace on first run
/// - Verify model integrity (SHA256)
/// - Lazy load model into memory
/// - Unload model on memory pressure
/// - Provide thread-safe access to loaded model
pub struct ModelManager {
    /// Configuration for the LLM model
    config: HookLlmConfig,

    /// Loaded model (lazy-loaded, wrapped in Arc for sharing)
    model: Arc<Mutex<Option<Arc<LlmModel>>>>,

    /// Model file path
    model_path: PathBuf,
}

impl ModelManager {
    /// Create a new model manager
    ///
    /// # Arguments
    ///
    /// * `config` - LLM configuration including model path and parameters
    ///
    /// # Errors
    ///
    /// Returns error if model path cannot be resolved or created
    pub async fn new(config: HookLlmConfig) -> Result<Self> {
        let model_path = expand_model_path(&config.model_path)?;

        info!(
            "Initializing model manager for {} at {:?}",
            config.model_name, model_path
        );

        Ok(Self {
            config,
            model: Arc::new(Mutex::new(None)),
            model_path,
        })
    }

    /// Ensure model is available locally
    ///
    /// Downloads the model from HuggingFace if it doesn't exist.
    /// This is called on daemon startup to prepare for classification.
    ///
    /// # Errors
    ///
    /// Returns error if download fails or model verification fails
    pub async fn ensure_model_available(&self) -> HookResult<()> {
        if !self.model_path_exists() {
            self.download_model().await?;
        } else {
            debug!("Model already exists at {:?}", self.model_path);
        }
        Ok(())
    }

    /// Check if model file exists on disk
    fn model_path_exists(&self) -> bool {
        self.model_path.exists()
    }

    /// Download model from HuggingFace
    ///
    /// # Implementation Notes
    ///
    /// - Uses streaming download with progress bar (indicatif)
    /// - Verifies SHA256 hash after download
    /// - Creates parent directories if needed
    ///
    /// # Errors
    ///
    /// Returns error if:
    /// - Network request fails
    /// - File write fails
    /// - Hash verification fails
    async fn download_model(&self) -> HookResult<()> {
        info!("Downloading model: {}", self.config.model_name);
        println!(
            "ðŸ“¥ Downloading CRUD classifier model (first-time setup, ~{}MB)...",
            self.config.model_size_mb
        );

        // Ensure parent directory exists
        if let Some(parent) = self.model_path.parent() {
            std::fs::create_dir_all(parent).map_err(|e| {
                HookError::execution_failed(
                    "model_download",
                    format!("Failed to create model directory: {}", e),
                )
            })?;
        }

        // Get download URL for TinyLLaMA
        let model_url = self.get_huggingface_url();

        // Create HTTP client
        let client = Client::new();
        let response = client
            .get(&model_url)
            .send()
            .await
            .map_err(|e| {
                HookError::execution_failed("model_download", format!("Failed to initiate download: {}", e))
            })?;

        if !response.status().is_success() {
            return Err(HookError::execution_failed(
                "model_download",
                format!("Download failed with status: {}", response.status()),
            ));
        }

        let total_size = response.content_length().unwrap_or(0);

        // Create progress bar
        let pb = ProgressBar::new(total_size);
        pb.set_style(
            ProgressStyle::default_bar()
                .template("{spinner:.green} [{elapsed_precise}] [{bar:40.cyan/blue}] {bytes}/{total_bytes} ({eta})")
                .unwrap()
                .progress_chars("#>-"),
        );

        // Download with progress tracking
        let mut file = tokio::fs::File::create(&self.model_path)
            .await
            .map_err(|e| {
                HookError::execution_failed("model_download", format!("Failed to create file: {}", e))
            })?;

        let mut downloaded: u64 = 0;
        let mut stream = response.bytes_stream();

        while let Some(chunk) = stream.next().await {
            let chunk = chunk.map_err(|e| {
                HookError::execution_failed("model_download", format!("Download stream error: {}", e))
            })?;

            tokio::io::AsyncWriteExt::write_all(&mut file, &chunk)
                .await
                .map_err(|e| {
                    HookError::execution_failed("model_download", format!("Failed to write file: {}", e))
                })?;

            downloaded += chunk.len() as u64;
            pb.set_position(downloaded);
        }

        pb.finish_with_message("Model downloaded");

        // Verify hash
        info!("Verifying model integrity...");
        self.verify_model_hash(&self.model_path)?;

        info!("Model download complete and verified");
        Ok(())
    }

    /// Get HuggingFace download URL for TinyLLaMA
    fn get_huggingface_url(&self) -> String {
        // TinyLLaMA 1.1B Chat Q4_K_M from HuggingFace
        "https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf".to_string()
    }

    /// Verify model file hash
    ///
    /// # Arguments
    ///
    /// * `path` - Path to the model file
    ///
    /// # Errors
    ///
    /// Returns error if hash doesn't match expected value
    fn verify_model_hash(&self, path: &Path) -> HookResult<()> {
        // Expected SHA256 hash for TinyLLaMA 1.1B Chat Q4_K_M
        // Note: This should be verified from the HuggingFace model card
        // For now, we'll skip strict hash verification and just log a warning
        // In production, you would verify against a known-good hash

        let file = std::fs::File::open(path).map_err(|e| {
            HookError::execution_failed("model_verification", format!("Failed to open model file: {}", e))
        })?;

        let mut reader = std::io::BufReader::new(file);
        let mut hasher = Sha256::new();
        let mut buffer = [0; 8192];

        loop {
            let count = std::io::Read::read(&mut reader, &mut buffer).map_err(|e| {
                HookError::execution_failed("model_verification", format!("Failed to read file: {}", e))
            })?;

            if count == 0 {
                break;
            }
            hasher.update(&buffer[..count]);
        }

        let computed_hash = hex::encode(hasher.finalize());

        // Log the hash for reference (users can verify against HuggingFace)
        info!("Model SHA256: {}", computed_hash);
        warn!("Skipping strict hash verification - please verify model integrity from HuggingFace");

        // In a production implementation, you would check:
        // if computed_hash != expected_hash {
        //     return Err(HookError::execution_failed(
        //         "model_verification",
        //         format!("Hash mismatch: got {}, expected {}", computed_hash, expected_hash)
        //     ));
        // }

        Ok(())
    }

    /// Load model into memory
    ///
    /// Lazy loads the model on first inference request.
    /// Uses tokio::spawn_blocking for the blocking I/O operation.
    ///
    /// # Errors
    ///
    /// Returns error if model file cannot be read or parsed
    pub async fn load_model(&self) -> HookResult<()> {
        let mut model_guard = self.model.lock().await;

        if model_guard.is_some() {
            debug!("Model already loaded");
            return Ok(());
        }

        info!("Loading model from {:?}", self.model_path);

        // Ensure model file exists
        if !self.model_path.exists() {
            return Err(HookError::execution_failed(
                "model_load",
                format!("Model file not found at {:?}", self.model_path),
            ));
        }

        // Load model in blocking task to avoid blocking async runtime
        let model_path = self.model_path.clone();
        let config = self.config.clone();

        let loaded = tokio::task::spawn_blocking(move || -> Result<Arc<LlmModel>, HookError> {
            // TODO: Integrate with llm crate once API is stable
            // For now, placeholder implementation that verifies the file exists

            info!("Model loading placeholder - actual LLM integration pending");
            info!("Model file: {:?}", model_path);

            // Verify model file exists and is readable
            if !model_path.exists() {
                return Err(HookError::execution_failed(
                    "model_load",
                    format!("Model file not found: {:?}", model_path),
                ));
            }

            let metadata = std::fs::metadata(&model_path).map_err(|e| {
                HookError::execution_failed("model_load", format!("Cannot read model file: {}", e))
            })?;

            info!("Model file size: {} MB", metadata.len() / (1024 * 1024));

            Ok(Arc::new(LlmModel {
                _model_path: model_path,
                _config: config,
            }))
        })
        .await
        .map_err(|e| {
            HookError::execution_failed("model_load", format!("Spawn blocking failed: {}", e))
        })??;

        *model_guard = Some(loaded);

        info!("Model loaded successfully");
        Ok(())
    }

    /// Unload model from memory
    ///
    /// Called when memory pressure is detected or during shutdown.
    /// The model will be lazy-loaded again on next inference request.
    pub async fn unload_model(&self) {
        let mut model_guard = self.model.lock().await;
        if model_guard.is_some() {
            info!("Unloading model from memory");
            *model_guard = None;
        }
    }

    /// Check if model is currently loaded
    pub async fn is_loaded(&self) -> bool {
        self.model.lock().await.is_some()
    }

    /// Get model configuration
    pub fn config(&self) -> &HookLlmConfig {
        &self.config
    }

    /// Get model file path
    pub fn model_path(&self) -> &Path {
        &self.model_path
    }

    /// Run inference
    ///
    /// # Arguments
    ///
    /// * `prompt` - The prompt to send to the model
    ///
    /// # Returns
    ///
    /// The model's response text
    ///
    /// # Errors
    ///
    /// Returns error if:
    /// - Model is not loaded
    /// - Inference fails
    /// - Timeout is exceeded
    pub async fn run_inference(&self, prompt: &str) -> HookResult<String> {
        // Ensure model is loaded
        if !self.is_loaded().await {
            self.load_model().await?;
        }

        // Get model reference
        let model_guard = self.model.lock().await;
        let _model = model_guard
            .as_ref()
            .ok_or_else(|| HookError::execution_failed("inference", "Model not loaded"))?
            .clone();
        drop(model_guard); // Release lock before blocking operation

        let prompt_str = prompt.to_string();

        // Run inference in blocking task
        let result = tokio::task::spawn_blocking(move || -> Result<String, HookError> {
            // TODO: Integrate with llm crate once API is stable
            // For now, placeholder implementation that returns a basic classification

            debug!("Inference placeholder - prompt: {}", prompt_str);
            warn!("Using placeholder inference - actual LLM integration pending");

            // CRITICAL FIX: Extract the actual command from the prompt
            // The prompt contains examples in the rules section, so we must parse
            // out just the "Command: <actual_command>" line to avoid false matches.
            let command = extract_command_from_prompt(&prompt_str);
            debug!("Extracted command: {}", command);

            if command.is_empty() {
                warn!("Failed to extract command from prompt, using fallback");
                return Ok("CREATE".to_string()); // Safe fallback
            }

            // Classify based on the EXTRACTED COMMAND only, not the full prompt
            let command_lower = command.to_lowercase();
            let command_trimmed = command_lower.trim();

            // READ: Operations that only retrieve/display data
            let classification = if is_read_operation(&command_trimmed) {
                "READ"
            // DELETE: Operations that remove resources
            } else if is_delete_operation(&command_trimmed) {
                "DELETE"
            // CREATE: Operations that make new resources
            } else if is_create_operation(&command_trimmed) {
                "CREATE"
            // UPDATE: Operations that modify existing resources
            } else if is_update_operation(&command_trimmed) {
                "UPDATE"
            } else {
                // Default to CREATE (safest - requires confirmation)
                "CREATE"
            };

            info!("Placeholder inference result: {} for command: {}", classification, command);
            Ok(classification.to_string())
        })
        .await
        .map_err(|e| {
            HookError::execution_failed("inference", format!("Spawn blocking failed: {}", e))
        })??;

        debug!("Inference result: {}", result);
        Ok(result)
    }
}

/// Expand tilde and environment variables in model path
fn expand_model_path(path: &Path) -> Result<PathBuf> {
    let path_str = path.to_string_lossy();

    // Expand tilde
    let expanded = if path_str.starts_with("~/") {
        if let Some(home) = dirs::home_dir() {
            home.join(&path_str[2..])
        } else {
            path.to_path_buf()
        }
    } else if path_str.starts_with('~') {
        // Handle ~user format (not commonly used, just use home dir)
        if let Some(home) = dirs::home_dir() {
            home.join(&path_str[1..])
        } else {
            path.to_path_buf()
        }
    } else {
        path.to_path_buf()
    };

    Ok(expanded)
}

#[cfg(test)]
mod tests {
    use super::*;
    use std::path::PathBuf;

    #[test]
    fn test_expand_model_path_tilde() {
        let path = PathBuf::from("~/.cco/models/test.gguf");
        let expanded = expand_model_path(&path).unwrap();

        // Should expand to home directory
        assert!(!expanded.to_string_lossy().contains('~'));
        assert!(expanded.to_string_lossy().contains(".cco/models/test.gguf"));
    }

    #[test]
    fn test_expand_model_path_absolute() {
        let path = PathBuf::from("/absolute/path/model.gguf");
        let expanded = expand_model_path(&path).unwrap();

        assert_eq!(expanded, path);
    }

    #[tokio::test]
    async fn test_model_manager_creation() {
        let config = HookLlmConfig {
            model_type: "tinyllama".to_string(),
            model_name: "test-model".to_string(),
            model_path: PathBuf::from("/tmp/test-model.gguf"),
            model_size_mb: 600,
            quantization: "Q4_K_M".to_string(),
            loaded: false,
            inference_timeout_ms: 2000,
            temperature: 0.1,
        };

        let manager = ModelManager::new(config).await.unwrap();
        assert!(!manager.is_loaded().await);
    }

    #[tokio::test]
    async fn test_model_unload() {
        let config = HookLlmConfig {
            model_type: "tinyllama".to_string(),
            model_name: "test-model".to_string(),
            model_path: PathBuf::from("/tmp/test-model.gguf"),
            model_size_mb: 600,
            quantization: "Q4_K_M".to_string(),
            loaded: false,
            inference_timeout_ms: 2000,
            temperature: 0.1,
        };

        let manager = ModelManager::new(config).await.unwrap();

        // Unload should be safe even if model not loaded
        manager.unload_model().await;
        assert!(!manager.is_loaded().await);
    }

    #[test]
    fn test_extract_command_from_prompt() {
        let prompt = r#"Classify this command as EXACTLY ONE of: READ, CREATE, UPDATE, or DELETE

Command: ls -la

Rules:
- READ: Retrieves/displays data, no side effects (ls, cat, grep, git status)
- CREATE: Makes new resources, files, processes (touch, mkdir, git init, docker run)"#;

        let extracted = extract_command_from_prompt(prompt);
        assert_eq!(extracted, "ls -la");

        // Test with different command
        let prompt2 = r#"Classify this command as EXACTLY ONE of: READ, CREATE, UPDATE, or DELETE

Command: rm -rf /tmp/test

Rules:
..."#;
        let extracted2 = extract_command_from_prompt(prompt2);
        assert_eq!(extracted2, "rm -rf /tmp/test");
    }

    #[test]
    fn test_read_operations() {
        // Basic read commands
        assert!(is_read_operation("ls -la"));
        assert!(is_read_operation("cat file.txt"));
        assert!(is_read_operation("grep pattern file"));
        assert!(is_read_operation("git status"));
        assert!(is_read_operation("git log --oneline"));
        assert!(is_read_operation("git diff HEAD~1"));
        assert!(is_read_operation("ps aux"));
        assert!(is_read_operation("find . -name '*.rs'"));
        assert!(is_read_operation("head -20 log.txt"));
        assert!(is_read_operation("tail -f application.log"));
        assert!(is_read_operation("docker ps -a"));
        assert!(is_read_operation("curl https://example.com"));

        // Piped read commands
        assert!(is_read_operation("cat file.txt | grep pattern | sort | uniq"));

        // Should NOT be read
        assert!(!is_read_operation("rm file.txt"));
        assert!(!is_read_operation("touch newfile.txt"));
    }

    #[test]
    fn test_create_operations() {
        assert!(is_create_operation("touch newfile.txt"));
        assert!(is_create_operation("mkdir -p path/to/dir"));
        assert!(is_create_operation("git init"));
        assert!(is_create_operation("git branch new-feature"));
        assert!(is_create_operation("docker run -d nginx"));
        assert!(is_create_operation("docker build -t myapp:latest ."));
        assert!(is_create_operation("npm init -y"));
        assert!(is_create_operation("cargo new my-project"));
        assert!(is_create_operation("echo 'hello' > output.txt"));
        assert!(is_create_operation("curl -o file.zip https://example.com/file.zip"));

        // Should NOT be create
        assert!(!is_create_operation("ls -la"));
        assert!(!is_create_operation("rm file.txt"));
    }

    #[test]
    fn test_update_operations() {
        assert!(is_update_operation("git commit -m 'Update README'"));
        assert!(is_update_operation("git add ."));
        assert!(is_update_operation("chmod +x script.sh"));
        assert!(is_update_operation("chown user:group file.txt"));
        assert!(is_update_operation("sed -i 's/old/new/g' file.txt"));
        assert!(is_update_operation("mv oldname.txt newname.txt"));
        assert!(is_update_operation("echo 'data' >> file.txt"));

        // Should NOT be update
        assert!(!is_update_operation("ls -la"));
        assert!(!is_update_operation("rm file.txt"));
    }

    #[test]
    fn test_delete_operations() {
        assert!(is_delete_operation("rm file.txt"));
        assert!(is_delete_operation("rm -rf directory/"));
        assert!(is_delete_operation("rmdir empty_directory"));
        assert!(is_delete_operation("docker rm container_name"));
        assert!(is_delete_operation("docker rmi image_name"));
        assert!(is_delete_operation("git branch -d feature-branch"));
        assert!(is_delete_operation("git clean -fd"));
        assert!(is_delete_operation("npm uninstall package-name"));

        // Should NOT be delete
        assert!(!is_delete_operation("ls -la"));
        assert!(!is_delete_operation("touch file.txt"));
    }
}
